{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project_fraud.lib import drop_many_missing_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = drop_many_missing_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert mail column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', \n",
    "          'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft',\n",
    "          'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo',\n",
    "          'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', \n",
    "          'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink',\n",
    "          'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other',\n",
    "          'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', \n",
    "          'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', \n",
    "          'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo',\n",
    "          'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other',\n",
    "          'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft',\n",
    "          'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', \n",
    "          'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', \n",
    "          'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', \n",
    "          'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', \n",
    "          'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', \n",
    "          'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', \n",
    "          'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other',\n",
    "          'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\n",
    "\n",
    "for c in ['P_emaildomain']:\n",
    "    data[c + '_bin'] = data[c].map(emails)\n",
    "    data[c + '_suffix'] = data[c].map(lambda x: str(x).split('.')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94456"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[c + '_bin'].isnull().sum()\n",
    "#print(data[c + '_suffix'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New feature: day of the week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new feature day of the week, encoded as 0-6 \n",
    "# found a good offset is 0.58\n",
    "\n",
    "def make_day_feature(data, offset=0, column_name='TransactionDT'):\n",
    "\n",
    "    days = data[column_name] / (3600*24)        \n",
    "    encoded_days = np.floor(days-1+offset) % 7\n",
    "    return encoded_days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New feature: hour of the day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new feature hour of the day, encoded as 0-23\n",
    "\n",
    "def make_hour_feature(data, column_name='TransactionDT'):\n",
    "\n",
    "    hours = data[column_name] / (3600)        \n",
    "    encoded_hours = np.floor(hours) % 24\n",
    "    return encoded_hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize periodicity: number of transactions per time interval \n",
    "\n",
    "vals = plt.hist(data['TransactionDT'] / (3600*24), bins=1800)\n",
    "plt.xlim(70, 78)\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Number of transactions')\n",
    "plt.ylim(0,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create new features: weekday and hour of day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new feature weekday\n",
    "# offset to define start of the day: 0.85\n",
    "\n",
    "data['weekday'] = make_day_feature(data, offset=0.58)\n",
    "\n",
    "plt.plot(data.groupby('weekday').mean()['isFraud'])\n",
    "\n",
    "plt.ylim(0, 0.04)\n",
    "plt.xlabel('Encoded day')\n",
    "plt.ylabel('Fraction of fraudulent transactions')\n",
    "\n",
    "# outputs fraction of fraudulent transactions per weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a feature which encodes the (relative) hour of the day\n",
    "\n",
    "data['hours'] = make_hour_feature(data)\n",
    "\n",
    "plt.plot(data.groupby('hours').mean()['isFraud'], color='k')\n",
    "\n",
    "ax = plt.gca()\n",
    "ax2 = ax.twinx()\n",
    "_ = ax2.hist(data['hours'], alpha=0.3, bins=24)\n",
    "ax.set_xlabel('Encoded hour')\n",
    "ax.set_ylabel('Fraction of fraudulent transactions')\n",
    "\n",
    "ax2.set_ylabel('Number of transactions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New features on each credit card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function(row):\n",
    "    if pd.isna(row['card1']):\n",
    "        return np.nan\n",
    "    elif pd.isna(row['card2']):\n",
    "        return np.nan\n",
    "    elif pd.isna(row['card3']):\n",
    "        return np.nan\n",
    "    elif pd.isna(row['card4']):\n",
    "        return np.nan\n",
    "    elif pd.isna(row['card5']):\n",
    "        return np.nan\n",
    "    elif pd.isna(row['card6']):\n",
    "        return np.nan\n",
    "    else: \n",
    "        return str(row['card1']) + str(row['card2']) + str(row['card3']) + str(row['card4']) + str(row['card5']) + str(row['card6'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cardID'] = data.apply(lambda row: function(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_cards = data.groupby('cardID').agg(\n",
    "    mean = pd.NamedAgg(column='TransactionAmt', aggfunc='mean'), \n",
    "    min = pd.NamedAgg(column='TransactionAmt', aggfunc='min'), \n",
    "    max = pd.NamedAgg(column='TransactionAmt', aggfunc='max'),\n",
    "    median = pd.NamedAgg(column='TransactionAmt', aggfunc='median'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.merge(credit_cards, how='left', on=\"cardID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Create new features:\n",
    "- Distance of current transaction from mean of transaction from credit card\n",
    "- Distance of current transaction from median of transaction from credit card\n",
    "- Relative distance of current transaction from mean of transaction from credit card\n",
    "- Relative distance of current transaction from median of transaction from credit card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_from_mean(row, metric): \n",
    "    if pd.isna(row['TransactionAmt']):\n",
    "        return np.nan\n",
    "    if pd.isna(row[metric]):\n",
    "        return np.nan\n",
    "    else: \n",
    "        dist = row['TransactionAmt'] - row[metric]\n",
    "        return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['dist_mean'] = data.apply(lambda row: dist_from_mean(row, 'mean'), axis=1)\n",
    "data['dist_median'] = data.apply(lambda row: dist_from_mean(row, 'median'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_from_median_rel(row, metric): \n",
    "    if pd.isna(row['TransactionAmt']):\n",
    "        return np.nan\n",
    "    if pd.isna(row[metric]):\n",
    "        return np.nan\n",
    "    else: \n",
    "        dist_rel = (row['TransactionAmt'] - row[metric]) / row[metric]\n",
    "        return dist_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['dist_mean_rel'] = data.apply(lambda row: dist_from_median_rel(row, 'mean'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['dist_median_rel'] = data.apply(lambda row: dist_from_median_rel(row, 'median'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder \n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPUTER AND ENCODING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new = data.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['TransactionID','P_emaildomain_suffix','P_emaildomain_bin',\n",
    "'card1','card2','addr1','TransactionAmt','card5','D15','C13','D2','D10','D4','TransactionDT','weekday','hours','cardID',\\\n",
    "          'mean','max','median','min', 'dist_mean', 'dist_median','dist_mean_rel','dist_median_rel']]\n",
    "y = data['isFraud']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Data without NAn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_new[['TransactionID',\n",
    "'card1','card2','addr1','TransactionAmt','card5','D15','C13','D2','D10','D4','TransactionDT',\\\n",
    "          'mean','max','median','min', 'dist_mean', 'dist_median','dist_mean_rel','dist_median_rel']]\n",
    "y = data_new['isFraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_new.where(data_new == 'nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = (X.dtypes != 'object')\n",
    "num_cols = list(n[n].index)\n",
    "medium_missing_num_cols = []\n",
    "low_missing_num_cols =[]\n",
    "for i in num_cols:\n",
    "    percentage = data[i].isnull().sum() * 100 / len(data[i])\n",
    "    if percentage < 15:\n",
    "        low_missing_num_cols.append(i)\n",
    "    elif percentage >= 15 and percentage <= 60:\n",
    "        medium_missing_num_cols.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_transformer_low = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())])\n",
    "num_transformer_medium = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "#cat_transformer = Pipeline([\n",
    "    #'imputer', SimpleImputer(strategy='constant', fill_value = \"Unknown\")\n",
    "    #])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Unknown')),\n",
    "    ('one_hot', OneHotEncoder())\n",
    "])\n",
    "    \n",
    "preprocessor = ColumnTransformer([\n",
    "    ('low_num_imputer',num_transformer_low, low_missing_num_cols),\n",
    "    ('medium_num_imputer', num_transformer_medium, medium_missing_num_cols),\n",
    "    ('cat_transformer', cat_pipeline, ['P_emaildomain_suffix','P_emaildomain_bin','weekday','hours', 'cardID'])],\n",
    "    remainder='passthrough')\n",
    "\n",
    "pd.DataFrame(preprocessor.fit_transform(X)).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Baseline Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3, random_state=414)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create smaller dataset for investigation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a smaller dataset for investigation purpose only\n",
    "sample_size = 20000\n",
    "\n",
    "X_small = X.sample(sample_size, random_state=0)\n",
    "y_small = y.sample(sample_size, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "log_model = LogisticRegression(class_weight='balanced')\n",
    "\n",
    "# Train the model on the training data\n",
    "log_model.fit(X_train, y_train)\n",
    "\n",
    "# Print the score of the model on the testing data\n",
    "log_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Basemodel: SGDClassifier Logistic Regression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "sgd_model = SGDClassifier(loss='log', alpha=0.5, class_weight='balanced')\n",
    "sgd_model.fit(X_train, y_train)\n",
    "\n",
    "cv_results_sgd_model = cross_val_score(sgd_model, X_train, y_train, cv=5, n_jobs=1, scoring='recall').mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_sgd_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Random Forest \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier(class_weight='balanced', random_state=0)\n",
    "\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "cv_results_forest = cross_validate(forest, X_train, y_train, cv=5, scoring='f1_macro')\n",
    "print(cv_results_forest['test_score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Permutation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation \n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "permutation_score = permutation_importance(log_model, X_train, y_train, n_repeats=10)\n",
    "\n",
    "np.vstack((X.columns, permutation_score.importances_mean)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and y\n",
    "X_best = df[['card2','TransactionAmt', 'TransactionDT', 'max', 'min', 'dist_mean']]\n",
    "y_best = df['gender_encoded']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3,random_state=2)\n",
    "\n",
    "log_model = LogisticRegression()\n",
    "\n",
    "log_model.fit(X_train,y_train)\n",
    "\n",
    "log_model.score(X_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
