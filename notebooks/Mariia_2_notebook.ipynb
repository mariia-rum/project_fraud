{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1097231, 254)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29015135\n"
     ]
    }
   ],
   "source": [
    "print(data.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert mail column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', \n",
    "          'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft',\n",
    "          'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo',\n",
    "          'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', \n",
    "          'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink',\n",
    "          'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other',\n",
    "          'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', \n",
    "          'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', \n",
    "          'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo',\n",
    "          'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other',\n",
    "          'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft',\n",
    "          'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', \n",
    "          'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', \n",
    "          'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', \n",
    "          'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', \n",
    "          'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', \n",
    "          'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', \n",
    "          'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other',\n",
    "          'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\n",
    "\n",
    "##us_emails = ['gmail',  'net',  'edu']\n",
    "\n",
    "# https://www.kaggle.com/c/ieee-fraud-detection/discussion/100499#latest-579654\n",
    "for c in ['P_emaildomain']:\n",
    "    data[c + '_bin'] = data[c].map(emails)\n",
    "   \n",
    "    \n",
    "    data[c + '_suffix'] = data[c].map(lambda x: str(x).split('.')[-1])\n",
    "    \n",
    "    \n",
    "   # df_train[c + '_suffix'] = df_train[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n",
    "    #df_test[c + '_suffix'] = df_test[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>TransactionDT</th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>ProductCD</th>\n",
       "      <th>card1</th>\n",
       "      <th>card2</th>\n",
       "      <th>card3</th>\n",
       "      <th>card4</th>\n",
       "      <th>card5</th>\n",
       "      <th>card6</th>\n",
       "      <th>...</th>\n",
       "      <th>id_33</th>\n",
       "      <th>id_34</th>\n",
       "      <th>id_35</th>\n",
       "      <th>id_36</th>\n",
       "      <th>id_37</th>\n",
       "      <th>id_38</th>\n",
       "      <th>DeviceType</th>\n",
       "      <th>DeviceInfo</th>\n",
       "      <th>P_emaildomain_bin</th>\n",
       "      <th>P_emaildomain_suffix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2987000.0</td>\n",
       "      <td>86400.0</td>\n",
       "      <td>68.5</td>\n",
       "      <td>W</td>\n",
       "      <td>13926.0</td>\n",
       "      <td>363.099769</td>\n",
       "      <td>150.0</td>\n",
       "      <td>discover</td>\n",
       "      <td>142.0</td>\n",
       "      <td>credit</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2987001.0</td>\n",
       "      <td>86401.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>W</td>\n",
       "      <td>2755.0</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>102.0</td>\n",
       "      <td>credit</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>google</td>\n",
       "      <td>com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   TransactionID  TransactionDT  TransactionAmt ProductCD    card1  \\\n",
       "0      2987000.0        86400.0            68.5         W  13926.0   \n",
       "1      2987001.0        86401.0            29.0         W   2755.0   \n",
       "\n",
       "        card2  card3       card4  card5   card6  ...  id_33  id_34  id_35  \\\n",
       "0  363.099769  150.0    discover  142.0  credit  ...    NaN    NaN    NaN   \n",
       "1  404.000000  150.0  mastercard  102.0  credit  ...    NaN    NaN    NaN   \n",
       "\n",
       "  id_36  id_37  id_38  DeviceType  DeviceInfo  P_emaildomain_bin  \\\n",
       "0   NaN    NaN    NaN         NaN         NaN                NaN   \n",
       "1   NaN    NaN    NaN         NaN         NaN             google   \n",
       "\n",
       "   P_emaildomain_suffix  \n",
       "0                   nan  \n",
       "1                   com  \n",
       "\n",
       "[2 rows x 256 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After converting the emails column we need to check the new amount of categorical columns, so we will will print a list of them\n",
    "and see missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c = (data.dtypes == 'object')\n",
    "categorical_cols = list(c[c].index)\n",
    "for i in categorical_cols:\n",
    "    print(data[i].value_counts())\n",
    "    print(i, \"missing values: \", data[i].isnull().sum()) \n",
    "    print(data[i].isnull().sum()*100/len(data[i]), \"\\n\") # missing percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ProductCD', 'card4', 'card6', 'P_emaildomain', 'M1', 'M2', 'M3', 'M4', 'M6', 'M7', 'M8', 'M9', 'id_12', 'id_15', 'id_16', 'id_28', 'id_29', 'id_30', 'id_31', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo', 'P_emaildomain_bin', 'P_emaildomain_suffix'] \n",
      "\n",
      "number categorical identity features:  29 \n",
      "\n",
      "\n",
      "['TransactionID', 'TransactionDT', 'TransactionAmt', 'card1', 'card2', 'card3', 'card5', 'addr1', 'addr2', 'dist1', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D10', 'D11', 'D15', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', 'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V95', 'V96', 'V97', 'V98', 'V99', 'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119', 'V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V126', 'V127', 'V128', 'V129', 'V130', 'V131', 'V132', 'V133', 'V134', 'V135', 'V136', 'V137', 'V279', 'V280', 'V281', 'V282', 'V283', 'V284', 'V285', 'V286', 'V287', 'V288', 'V289', 'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V296', 'V297', 'V298', 'V299', 'V300', 'V301', 'V302', 'V303', 'V304', 'V305', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', 'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06', 'id_09', 'id_10', 'id_11', 'id_13', 'id_14', 'id_17', 'id_19', 'id_20', 'id_32'] \n",
      "\n",
      "number numerical identity features:  227\n"
     ]
    }
   ],
   "source": [
    "# create a list of numerical features\n",
    "# create a list of categorical features\n",
    "\n",
    "c = (data.dtypes == 'object')\n",
    "n = (data.dtypes != 'object')\n",
    "cat_id_cols = list(c[c].index)\n",
    "num_id_cols = list(n[n].index) \n",
    "\n",
    "print(cat_id_cols, \"\\n\")\n",
    "print(\"number categorical identity features: \", len(cat_id_cols), \"\\n\\n\")\n",
    "print(num_id_cols, \"\\n\")\n",
    "print(\"number numerical identity features: \", len(num_id_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIST OF FEATURES TO USE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First list of features \n",
    "\n",
    "\n",
    "### from this notebook https://www.kaggle.com/kabure/extensive-eda-and-modeling-xgb-hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data['TransactionID'] #noice\n",
    "data['card1']  #noice\n",
    "data['card2']   #noice  #this one can be combined with ['addr1']\n",
    "data['TransactionAmt_to_mean_card1']\n",
    "data['addr1']    #noice\n",
    "data['TransactionAmt_to_std_card1'] #noice\n",
    "data['TransactionAmt'] #noice\n",
    "data['PCA_V_13'] \n",
    "data['TransactionAmt_to_std_card4']#noice\n",
    "data['card5']  #in other data set was used with mean \n",
    "data['D8'] #noice\n",
    "data['TransactionAmt_to_mean_card4']#noice\n",
    "data['PCA_V_29']\n",
    "data['PCA_V_14']\n",
    "data['PCA_V_27']\n",
    "data['PCA_V_23']\n",
    "data['dist1'] #noice\n",
    "data['PCA_V_17']\n",
    "data['D15']  #noice\n",
    "data['PCA_V_26']\n",
    "data['P_emaildomain_bin'] #engineered by us \n",
    "data['P_emaildomain_suffix']#engineered by us "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second list of features \n",
    "\n",
    "\n",
    "### from this notebook https://www.kaggle.com/plasticgrammer/ieee-cis-fraud-detection-eda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['card1']\n",
    "data['_card2__addr1']\n",
    "data['addr1']\n",
    "data['_weekday__hour']\n",
    "data['_count__card_all__addr1']\n",
    "data['_count_rate']\n",
    "data['_card1__card2']\n",
    "data['_count_card1']\n",
    "data['card2']\n",
    "data['_amount_pct__card_all__addr1']\n",
    "data['_P_emaildomain__addr1']\n",
    "data['_card1_addr1']\n",
    "data['_amount_fraction']\n",
    "data['D15']\n",
    "data['_amount_mean_P_emaildomain']\n",
    "data['_count_card2']\n",
    "data['_amount_std_P_emaildomain']\n",
    "data['_vcol_pca0']\n",
    "data['_amount_pct_card1']\n",
    "data['_hour']\n",
    "data['dist1']\n",
    "data['C13']\n",
    "data['D2']\n",
    "data['D10']\n",
    "data['D4']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLAN FOR TOMORROW FOR FEATURES \n",
    "## - generate features to std and to mean if we want to use them \n",
    "## - generate features to combine card and adress if we want to \n",
    "## - PCA Not sure \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURES MEAN STD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the outlier rows with standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Trans_min_mean'] = data['TransactionAmt'] - data['TransactionAmt'].mean()\n",
    "data['Trans_min_std'] = data['Trans_min_mean'] / data['TransactionAmt'].std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['TransactionAmt_to_mean_card1'] = data['TransactionAmt'] / data.groupby(['card1'])['TransactionAmt'].transform('mean')\n",
    "data['TransactionAmt_to_mean_card4'] = data['TransactionAmt'] / data.groupby(['card4'])['TransactionAmt'].transform('mean')\n",
    "data['TransactionAmt_to_std_card1'] = data['TransactionAmt'] / data.groupby(['card1'])['TransactionAmt'].transform('std')\n",
    "data['TransactionAmt_to_std_card4'] = data['TransactionAmt'] / data.groupby(['card4'])['TransactionAmt'].transform('std')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log transformation helps to handle skewed data and after transformation, the distribution becomes more approximate to normal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['TransactionAmt'] = np.log(data['TransactionAmt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To check missimg values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[col].isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "aspiration_encoder = LabelEncoder()\n",
    "\n",
    "data[\"col_name\"] = aspiration_encoder.fit_transform(data['col_name'])\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Initiate imputer with desired strategy\n",
    "imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "# Call the method \"fit\" on the object\n",
    "imputer.fit(data[['col_name']])\n",
    "\n",
    "#Call the method \"transform\" on the object\n",
    "data['col_name'] = imputer.transform(data[['col_name']])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and y\n",
    "X = data.drop(columns=['isFraud'])\n",
    "y = data['isFraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a smaller dataset for investigation purpose only\n",
    "sample_size = 20000\n",
    "tmp = data.sample(sample_size, random_state=414)\n",
    "X_small = tmp.drop(columns=['isFraud'])\n",
    "y_small = tmp['isFraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split using random_state=414\n",
    "# (let's forget for the sake of this challenge that we are data-leaking a bit here, we should have done the split earlier)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=414)\n",
    "X_train_small, X_test_small, y_train_small, y_test_small = train_test_split(X_small, y_small, random_state=414)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (optional) Create here an train/eval split within the train set itself.\n",
    "# Some powerfull models (XGBOOST, Neural Network...) which are prone to overfitting on the traning set, needs \"early stopping criteria\", to avoid descending the gradient completely and avoid overfitting.\n",
    "X_train_train, X_train_test, y_train_train, y_train_test = train_test_split(X_train, y_train)\n",
    "X_train_train_small, X_train_test_small, y_train_train_small, y_train_test_small = train_test_split(X_train_small, y_train_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# BASELINE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Basemodel: SGDClassifier Logistic Regression \n",
    "\n",
    "log_reg_model = LogisticRegression(class_weight='balanced')\n",
    "cross_val_score(log_reg_model, X_train, y_train, cv=3, scoring='recall')\n",
    "\n",
    "base_model = SGDClassifier(loss='log', alpha=0.5, class_weight='balanced')\n",
    "cv_results_base_model = cross_validate(base_model, X_train, y_train, cv=5, n_jobs=1, scoring=['recall', 'f1_macro'])\n",
    "cv_results_base_model['test_f1_macro'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Model \n",
    "\n",
    "log_reg_model = LogisticRegression(class_weight='balanced')\n",
    "cv_results_log_reg_model = cross_val_score(log_reg_model, X_train, y_train, cv=5, scoring=['recall', 'f1_macro'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concating dfs to get PCA of V features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_change(data, col, n_components, prefix='PCA_', rand_seed=4):\n",
    "    pca = PCA(n_components=n_components, random_state=rand_seed)\n",
    "\n",
    "    principalComponents = pca.fit_transform(data[col])\n",
    "\n",
    "    principalDf = pd.DataFrame(principalComponents)\n",
    "\n",
    "    data.drop(cols, axis=1, inplace=True)\n",
    "\n",
    "    principalDf.rename(columns=lambda x: str(prefix)+str(x), inplace=True)\n",
    "\n",
    "    data = pd.concat([data, principalDf], axis=1)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Instanciate the model\n",
    "knn_model = KNeighborsClassifier(n_neighbors=10)\n",
    "\n",
    "# Train the model on the Training data\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Score the model on the Testing data\n",
    "knn_model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(1,25),score,color='blue', linestyle='dashed', marker='o',markerfacecolor='red', markersize=10)\n",
    "plt.title('Score vs. K Neighbors')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop over different values of K and record the model's score for each value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = []\n",
    "\n",
    "for k in range(1,25):\n",
    "    \n",
    "    # Instanciate the model\n",
    "    knn_model = KNeighborsClassifier(n_neighbors = k)\n",
    "\n",
    "    # Train the model on the scaled Training data\n",
    "    knn_model.fit(X_train, y_train)\n",
    "\n",
    "    # Append the score \n",
    "    score.append(knn_model.score(X_test,y_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(1,25),score,color='blue', linestyle='dashed', marker='o',markerfacecolor='red', markersize=10)\n",
    "plt.title('Score vs. K Neighbors')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to see which value of K performs best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.argmax(score)+1 # +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "cross_validate(knn, X_train_scaled, y_train, cv=5, scoring='roc_auc')[\"test_score\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Grid search\n",
    "\n",
    "Use KNeighborsClassifier\n",
    "\n",
    "ðŸ‘‡ Grid search a KNN's hyperparameter k on the training data.\n",
    "- Search k = [1,5,10,20]\n",
    "- 5-fold cross validate\n",
    "- Score with recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Instanciate model\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "# Hyperparameter Grid\n",
    "k_grid = {'n_neighbors' : [1, 5,10,20]}\n",
    "\n",
    "# Instanciate Grid Search\n",
    "grid = GridSearchCV(model, k_grid, n_jobs=-1, scoring = 'roc_auc', cv = 5)\n",
    "\n",
    "# Fit data to Grid Search\n",
    "grid.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the best model from the grid search and score its performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "model = grid.best_estimator_\n",
    "roc_auc_score(model.predict(scaler.transform(X_test)),y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "search_space = {'n_neighbors': randint(1, 40), 'p': [1, 2]}\n",
    "\n",
    "search = RandomizedSearchCV(model, param_distributions=search_space,\n",
    "                            n_jobs=-1, scoring='roc_auc', cv=5, n_iter=10)\n",
    "\n",
    "search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(search.best_score_)\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Classifier for non-linearly separable data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy import stats\n",
    "\n",
    "# Instanciate model\n",
    "model = SVC()\n",
    "\n",
    "# Hyperparameter search space\n",
    "search_space = {\n",
    "    'kernel': ['sigmoid'],\n",
    "    'C': stats.uniform(0.01, 1000),\n",
    "    'gamma': stats.loguniform(0.001,10),\n",
    "    'coef0': stats.uniform(-5,5),\n",
    "}\n",
    "\n",
    "# Instanciate Random Search\n",
    "rsearch = RandomizedSearchCV(\n",
    "    model, search_space,\n",
    "    n_jobs=-1, scoring='accuracy', cv=5, n_iter=1000, verbose=1)\n",
    "\n",
    "\n",
    "rsearch.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rsearch.best_params_)\n",
    "print(rsearch.best_score_)\n",
    "best_svm = rsearch.best_estimator_.fit(X,y)\n",
    "plot_decision_regions(X, y, classifier=best_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "print('CROSS VALIDATED RESULT')\n",
    "print('mean accuracy', cross_val_score(best_svm, X, y, cv=10).mean())\n",
    "print('std', cross_val_score(rsearch.best_estimator_, X, y, cv=10).std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor as vif\n",
    "df = pd.DataFrame()\n",
    "df[\"vif_index\"] = [vif(Xp, i) for i in range(Xp.shape[1])]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
